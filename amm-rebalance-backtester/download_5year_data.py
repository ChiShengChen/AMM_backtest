#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è¼‰5å¹´æ—¥ç·šæ•¸æ“šè…³æœ¬
æ”¯æŒ ETH/USDC, BTC/USDC, USDT/USDC
"""

import os
import sys
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
import logging
from pathlib import Path

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download_5year_data.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BinanceDataDownloader:
    """Binance æ•¸æ“šä¸‹è¼‰å™¨"""
    
    def __init__(self):
        self.base_url = "https://api.binance.com"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "AMMBacktester/1.0"
        })
        
        # æ”¯æŒçš„äº¤æ˜“å°
        self.symbols = {
            "ETHUSDC": "ETHUSDC",
            "BTCUSDC": "BTCUSDC", 
            "USDCUSDT": "USDCUSDT"  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„ Binance äº¤æ˜“å°åç¨±
        }
        
        # æ™‚é–“é–“éš”æ˜ å°„
        self.interval = "1d"
        
    def fetch_klines(self, symbol: str, start_time: int, end_time: int, limit: int = 1000) -> list:
        """ç²å– K ç·šæ•¸æ“š"""
        params = {
            "symbol": symbol,
            "interval": self.interval,
            "startTime": start_time,
            "endTime": end_time,
            "limit": limit
        }
        
        try:
            response = self.session.get(
                f"{self.base_url}/api/v3/klines",
                params=params,
                timeout=30
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"ç²å– {symbol} æ•¸æ“šå¤±æ•—: {e}")
            return []
    
    def download_symbol_data(self, symbol: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """ä¸‹è¼‰æŒ‡å®šäº¤æ˜“å°çš„å®Œæ•´æ•¸æ“š"""
        logger.info(f"é–‹å§‹ä¸‹è¼‰ {symbol} å¾ {start_date.date()} åˆ° {end_date.date()}")
        
        # è½‰æ›ç‚ºæ¯«ç§’æ™‚é–“æˆ³
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        all_klines = []
        current_start = start_ts
        
        while current_start < end_ts:
            current_end = min(current_start + (1000 * 24 * 60 * 60 * 1000), end_ts)  # 1000å¤©
            
            logger.info(f"ä¸‹è¼‰ {symbol} å¾ {datetime.fromtimestamp(current_start/1000).date()} åˆ° {datetime.fromtimestamp(current_end/1000).date()}")
            
            klines = self.fetch_klines(symbol, current_start, current_end, 1000)
            
            if not klines:
                logger.warning(f"æ²’æœ‰ç²å–åˆ° {symbol} çš„æ•¸æ“š")
                break
            
            all_klines.extend(klines)
            
            # æ›´æ–°é–‹å§‹æ™‚é–“
            current_start = current_end + 1
            
            # é€Ÿç‡é™åˆ¶ï¼š1200 requests/minï¼Œæˆ‘å€‘ä¿å®ˆä¸€é»
            time.sleep(0.1)
        
        if not all_klines:
            logger.error(f"æ²’æœ‰ç²å–åˆ° {symbol} çš„ä»»ä½•æ•¸æ“š")
            return pd.DataFrame()
        
        # è½‰æ›ç‚º DataFrame
        df = pd.DataFrame(all_klines, columns=[
            "timestamp", "open", "high", "low", "close", "volume",
            "close_time", "quote_volume", "trades", "taker_buy_base",
            "taker_buy_quote", "ignore"
        ])
        
        # è½‰æ›æ•¸æ“šé¡å‹
        numeric_columns = ["open", "high", "low", "close", "volume", "quote_volume", "trades", "taker_buy_base", "taker_buy_quote"]
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # è½‰æ›æ™‚é–“æˆ³
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
        df["close_time"] = pd.to_datetime(df["close_time"], unit="ms")
        
        # æ’åºä¸¦å»é‡
        df = df.sort_values("timestamp").drop_duplicates(subset=["timestamp"]).reset_index(drop=True)
        
        logger.info(f"æˆåŠŸä¸‹è¼‰ {symbol}: {len(df)} æ¢è¨˜éŒ„")
        return df
    
    def save_data(self, df: pd.DataFrame, symbol: str, output_dir: str):
        """ä¿å­˜æ•¸æ“šåˆ°æ–‡ä»¶"""
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        start_date = df["timestamp"].min().strftime("%Y%m%d")
        end_date = df["timestamp"].max().strftime("%Y%m%d")
        filename = f"{symbol}_1d_{start_date}_{end_date}.csv"
        filepath = os.path.join(output_dir, filename)
        
        # ä¿å­˜åˆ° CSV
        df.to_csv(filepath, index=False)
        logger.info(f"æ•¸æ“šå·²ä¿å­˜åˆ°: {filepath}")
        
        # é¡¯ç¤ºæ•¸æ“šæ‘˜è¦
        logger.info(f"æ•¸æ“šæ‘˜è¦:")
        logger.info(f"  æ™‚é–“ç¯„åœ: {df['timestamp'].min()} åˆ° {df['timestamp'].max()}")
        logger.info(f"  è¨˜éŒ„æ•¸é‡: {len(df)}")
        logger.info(f"  åƒ¹æ ¼ç¯„åœ: ${df['low'].min():.2f} - ${df['high'].max():.2f}")
        logger.info(f"  ç¸½äº¤æ˜“é‡: {df['volume'].sum():.2f}")
        
        return filepath

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description='æ‰¹é‡ä¸‹è¼‰5å¹´æ—¥ç·šæ•¸æ“š')
    parser.add_argument('--symbol', help='æŒ‡å®šäº¤æ˜“å° (å¯é¸ï¼Œä¸æŒ‡å®šå‰‡ä¸‹è¼‰æ‰€æœ‰)')
    parser.add_argument('--start', help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DDï¼Œå¯é¸)')
    parser.add_argument('--end', help='çµæŸæ—¥æœŸ (YYYY-MM-DDï¼Œå¯é¸)')
    parser.add_argument('--out', help='è¼¸å‡ºç›®éŒ„ (å¯é¸)')
    
    args = parser.parse_args()
    
    # å‰µå»ºä¸‹è¼‰å™¨
    downloader = BinanceDataDownloader()
    
    # è¨­ç½®æ™‚é–“ç¯„åœ
    if args.end:
        end_date = datetime.strptime(args.end, '%Y-%m-%d')
    else:
        end_date = datetime.now()
        
    if args.start:
        start_date = datetime.strptime(args.start, '%Y-%m-%d')
    else:
        start_date = end_date - timedelta(days=5*365)  # 5å¹´å‰
    
    logger.info(f"ä¸‹è¼‰æ™‚é–“ç¯„åœ: {start_date.date()} åˆ° {end_date.date()}")
    
    # å‰µå»ºæ•¸æ“šç›®éŒ„
    if args.out:
        data_dir = args.out
    else:
        data_dir = "data/5year_daily"
    os.makedirs(data_dir, exist_ok=True)
    
    # ç¢ºå®šè¦ä¸‹è¼‰çš„äº¤æ˜“å°
    if args.symbol:
        # ä¸‹è¼‰æŒ‡å®šäº¤æ˜“å°
        if args.symbol in downloader.symbols:
            symbols_to_download = {args.symbol: downloader.symbols[args.symbol]}
        else:
            logger.error(f"ä¸æ”¯æŒçš„äº¤æ˜“å°: {args.symbol}")
            return
    else:
        # ä¸‹è¼‰æ‰€æœ‰äº¤æ˜“å°
        symbols_to_download = downloader.symbols
    
    # ä¸‹è¼‰æ•¸æ“š
    success_count = 0
    total_count = len(symbols_to_download)
    
    for symbol_name, symbol_code in symbols_to_download.items():
        try:
            logger.info(f"æ­£åœ¨è™•ç† {symbol_name} ({symbol_code})")
            
            # ä¸‹è¼‰æ•¸æ“š
            df = downloader.download_symbol_data(symbol_code, start_date, end_date)
            
            if not df.empty:
                # ä¿å­˜æ•¸æ“š
                filepath = downloader.save_data(df, symbol_code, data_dir)
                success_count += 1
                logger.info(f"âœ… {symbol_name} ä¸‹è¼‰æˆåŠŸ")
            else:
                logger.error(f"âŒ {symbol_name} ä¸‹è¼‰å¤±æ•—ï¼šæ²’æœ‰æ•¸æ“š")
                
        except Exception as e:
            logger.error(f"âŒ {symbol_name} ä¸‹è¼‰å¤±æ•—: {e}")
        
        # åœ¨äº¤æ˜“å°ä¹‹é–“æ·»åŠ å»¶é²
        if symbol_name != list(symbols_to_download.keys())[-1]:
            logger.info("ç­‰å¾… 5 ç§’å¾Œç¹¼çºŒä¸‹ä¸€å€‹äº¤æ˜“å°...")
            time.sleep(5)
    
    # ç¸½çµå ±å‘Š
    logger.info("="*60)
    logger.info("ä¸‹è¼‰å®Œæˆç¸½çµ")
    logger.info("="*60)
    logger.info(f"ç¸½äº¤æ˜“å°æ•¸: {total_count}")
    logger.info(f"æˆåŠŸä¸‹è¼‰: {success_count}")
    logger.info(f"å¤±æ•—æ•¸é‡: {total_count - success_count}")
    logger.info(f"æ•¸æ“šä¿å­˜ç›®éŒ„: {data_dir}")
    
    if success_count == total_count:
        logger.info("ğŸ‰ æ‰€æœ‰äº¤æ˜“å°æ•¸æ“šä¸‹è¼‰æˆåŠŸï¼")
    else:
        logger.warning("âš ï¸ éƒ¨åˆ†äº¤æ˜“å°æ•¸æ“šä¸‹è¼‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
    
    # é¡¯ç¤ºä¸‹è¼‰çš„æ–‡ä»¶
    logger.info("\nä¸‹è¼‰çš„æ–‡ä»¶:")
    for file in os.listdir(data_dir):
        if file.endswith('.csv'):
            filepath = os.path.join(data_dir, file)
            size = os.path.getsize(filepath) / 1024  # KB
            logger.info(f"  {file} ({size:.1f} KB)")

if __name__ == "__main__":
    main()
